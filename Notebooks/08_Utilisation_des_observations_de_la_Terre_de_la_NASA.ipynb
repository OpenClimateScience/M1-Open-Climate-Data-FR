{
    "cells": [
        {
            "cell_type": "markdown",
            "id": "a79bde19-ec9b-401e-8ad0-6bfad1ad6dc3",
            "metadata": {},
            "source": [
                "# M1.8 - Utilisation des Observations Terrestres de la NASA"
            ]
        },
        {
            "cell_type": "markdown",
            "id": "0d24afa7-54b1-45bc-ab6b-9164cab19231",
            "metadata": {},
            "source": [
                "*Partie de:* [**Donn\u00e9es Climatiques Ouvertes**](https://github.com/OpenClimateScience/M1-Open-Climate-Data) | **Le\u00e7on Pr\u00e9c\u00e9dente** | **Le\u00e7on Suivante**\n",
                "\n",
                "**Contenu :**\n",
                "\n",
                "- [Organisation de notre syst\u00e8me de fichiers](#Organisation-de-notre-syst\u00e8me-de-fichiers)\n",
                "- [Utilisation des observations climatiques de la NASA](#Utilisation-des-observations-climatiques-de-la-NASA)\n",
                "  - [T\u00e9l\u00e9chargement des donn\u00e9es d'humidit\u00e9 du sol de niveau 3 de SMAP](#T\u00e9l\u00e9chargement-des-donn\u00e9es-d'humidit\u00e9-du-sol-de-niveau-3-de-SMAP)\n",
                "  - [Personnalisation d'un t\u00e9l\u00e9chargement via Earthdata Search](#Personnalisation-d'un-t\u00e9l\u00e9chargement-via-Earthdata-Search)\n",
                "- [Comprendre les fichiers de donn\u00e9es hi\u00e9rarchiques (HDF5)](#Comprendre-les-fichiers-de-donn\u00e9es-hi\u00e9rarchiques-(HDF5))\n",
                "  - [Lecture des ensembles de donn\u00e9es HDF5](#Lecture-des-ensembles-de-donn\u00e9es-HDF5)\n",
                "  - [Extraction des donn\u00e9es SMAP L3](#Extraction-des-donn\u00e9es-SMAP-L3)\n",
                "- [Masquage des plans d'eau permanents](#Masquage-des-plans-d'eau-permanents)\n",
                "- [Utilisation des indicateurs d'assurance qualit\u00e9](#Utilisation-des-indicateurs-d'assurance-qualit\u00e9)\n",
                "- [Cr\u00e9ation d'une s\u00e9rie chronologique d'humidit\u00e9 du sol](#Cr\u00e9ation-d'une-s\u00e9rie-chronologique-d'humidit\u00e9-du-sol)\n",
                "  - [Calcul d'une moyenne mobile](#Calcul-d'une-moyenne-mobile)\n",
                "\n",
                "---"
            ]
        }
    ],
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c13a88-9869-4dad-8384-e863e384589c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import earthaccess\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from matplotlib import pyplot\n",
    "\n",
    "auth = earthaccess.login()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38f7b524-c0a6-46d7-9a8c-255d2a9a5c55",
   "metadata": {},
   "source": [
    "## Organisation de notre système de fichiers\n",
    "\n",
    "Encore une fois, nous aurons besoin d'un endroit pour stocker ces données brutes. Lorsque nous avons commencé à travailler avec les données de Noah NLDAS, nous avons créé les dossiers suivants dans notre système de fichiers :\n",
    "\n",
    "```\n",
    "data_raw/\n",
    "  NLDAS\n",
    "  SMAP_L3\n",
    "```\n",
    "\n",
    "Assurez-vous qu'il y ait également un dossier `SMAP_L3` pour recevoir les données que nous sommes sur le point de télécharger !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1762ecbd-552c-4892-a296-2292906776a5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Utilisation des observations climatiques de la NASA\n",
    "\n",
    "Les données NLDAS que nous avons utilisées sont un excellent outil pour des études rétrospectives mais, en tant que données de réanalyse, elles présentent certaines limitations :\n",
    "\n",
    "- Elles ont une latence relativement élevée ; il peut s'écouler des jours ou des semaines avant que les données ne soient disponibles.\n",
    "- Elles intègrent des données provenant de multiples sources mais avec des niveaux d'exactitude et une couverture géographique variables.\n",
    "\n",
    "Si nous voulons caractériser la sécheresse éclair ou la détecter en quasi temps réel, nous ne devons pas utiliser des ensembles de données de réanalyse. Au lieu de cela, nous voulons des observations directes des conditions de sécheresse. **Voyons ce que nous pouvons apprendre sur la sécheresse éclair de 2017 à partir des estimations d'humidité du sol par satellite de la NASA.**\n",
    "\n",
    "**Nous allons utiliser les données de la mission Soil Moisture Active Passive (SMAP) de la NASA.** [Les missions d'observation de la Terre de la NASA fournissent des données regroupées en différents niveaux de traitement :](https://www.earthdata.nasa.gov/engage/open-data-services-and-software/data-information-policy/data-levels)\n",
    "\n",
    "- **Niveau 1 (Données brutes) :** Il s'agit essentiellement des valeurs mesurées directement par un instrument satellitaire. Elles peuvent ou non être interprétables physiquement. La plupart des utilisateurs finaux ne bénéficieront pas des données de niveau 1.\n",
    "- **Niveau 2 :** Ce sont des valeurs physiquement interprétables dérivées des données brutes, avec la même résolution spatiale et temporelle que les données de niveau 1. Les données de niveau 2 peuvent être difficiles à utiliser car la structure spatiale des données correspond à la géométrie de vision de l'instrument.\n",
    "- **Niveau 3 :** Au niveau 3, les valeurs géophysiques ont été standardisées sur une grille spatiale uniforme et une série chronologique uniforme. Bien que certaines valeurs puissent être manquantes en raison de la faible qualité, des nuages ou de la défaillance du capteur, les données de niveau 3 en grille à différentes étapes temporelles peuvent être facilement combinées et comparées.\n",
    "- **Niveau 4 (Données améliorées par modèle) :** Au niveau 4, les valeurs des données de niveau 3 sont intégrées dans un modèle, combinant éventuellement des ensembles de données supplémentaires et indépendants provenant d'autres capteurs pour produire des estimations ou des analyses améliorées des variables géophysiques.\n",
    "\n",
    "### Téléchargement des données d'humidité du sol de niveau 3 de SMAP\n",
    "\n",
    "**[Nous utiliserons les données d'humidité du sol de surface de niveau 3 à 36 km de la mission SMAP](https://nsidc.org/data/spl3smp/versions/8)** car elles représentent un bon compromis entre les observations directes des capteurs et la facilité d'utilisation.\n",
    "\n",
    "- Sur le site web ci-dessus, nous pouvons voir qu'il existe plusieurs façons d'accéder aux données. [Utilisons Earthdata Search ;](https://search.earthdata.nasa.gov/search?q=SPL3SMP+V008) pouvons-nous accéder aux données depuis le cloud de la NASA en utilisant `earthaccess` ?\n",
    "- Vous avez peut-être remarqué que les données de niveau 3 de SMAP que nous voulons utiliser *ne sont pas* « Disponibles dans le cloud Earthdata ». Il semble que nous devions télécharger les données directement.\n",
    "- **Où allons-nous mettre les données brutes que nous téléchargeons ?** Revisitons notre arbre de fichiers dans Jupyter Notebook.\n",
    "- **Dans le dossier `data_raw`, créons un nouveau dossier appelé `SMAP_L3`.** C'est ici que nous placerons les données que nous allons télécharger.\n",
    "\n",
    "Nous avons discuté de l'importance d'avoir un flux de travail bien documenté qui permet de comprendre facilement comment nous avons obtenu un résultat scientifique particulier. Nous supposons que nous pouvons re-télécharger les données brutes que nous avons utilisées à tout moment, mais que se passerait-il si nous oublions d'où viennent les données ? Étant donné que les données de niveau 3 de SMAP ne sont pas disponibles dans le cloud, nous allons les télécharger manuellement et il serait bon de documenter les étapes que nous avons suivies pour le faire, au cas où des questions se poseraient sur l'origine des données ou le type de traitement qui leur a été appliqué."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e968705-497e-42e6-98cc-fb44cbfbf7a8",
   "metadata": {},
   "source": [
    "#### &#x1F3AF; Bonne Pratique\n",
    "\n",
    "- Dans l'arborescence du fichier Jupyter Notebook, dans le dossier `SMAP_L3`, créons un `\"Nouveau Fichier\"`. Nommez le nouveau fichier texte `README.txt`.\n",
    "- Double-cliquez sur `README.txt` pour l'ouvrir. C'est ici que nous ajouterons des informations utiles sur les données que nous allons télécharger. Voici un exemple.\n",
    "\n",
    "```\n",
    "Auteur : Jane Q. Public (jane.public@example.com)\n",
    "Date : 1er novembre 2023\n",
    "\n",
    "Ce dossier contient les données de niveau 3 de la mission SMAP. Elles ont été téléchargées depuis :\n",
    "\n",
    "    https://search.earthdata.nasa.gov/search?q=SPL3SMP+V008\n",
    "\n",
    "Voici quelques informations supplémentaires sur ce produit :\n",
    "\n",
    "    https://nsidc.org/data/spl3smp/versions/8\n",
    "```\n",
    "\n",
    "Cela peut ne pas sembler beaucoup d'informations, mais il y a suffisamment ici que nous voudrions savoir si nous faisions une longue pause dans ce projet ou si quelqu'un d'autre devait essayer de comprendre ce que nous faisions. Et sa courte longueur est également un avantage : **documenter votre projet ne doit pas être difficile et toute quantité d'informations est meilleure que rien.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50fe9d41-9103-4ba6-add7-a9fbc08ea795",
   "metadata": {},
   "source": [
    "### Personnalisation d'un téléchargement via Earthdata Search\n",
    "\n",
    "Le satellite SMAP effectue deux passages chaque jour, un passage \"matinal\" et un passage \"après-midi\" (heure locale). Utilisons les données d'humidité du sol du passage après-midi (PM), car c'est probablement à ce moment que le stress hydrique sur la végétation est à son maximum.\n",
    "\n",
    "- [**Ce lien vous mènera à la bonne page pour commencer.**](https://search.earthdata.nasa.gov/search?q=SPL3SMP%20V008) Cliquez sur l'ensemble de données qui est affiché à droite de la fenêtre de recherche.\n",
    "- Nous téléchargerons les données d'août et septembre pour étudier l'apparition et la progression de la sécheresse éclair de 2017 : **Choisissez une plage temporelle, du 1er juin 2017 au 30 septembre 2017.**\n",
    "- En bas à droite, **cliquez sur le gros bouton vert qui indique \"Download All\" (Tout Télécharger).**\n",
    "- 3,8 Go, c'est beaucoup de données ! Pouvons-nous rendre ce téléchargement plus petit ? Nous ne sommes intéressés que par l'humidité du sol du passage après-midi. **Cliquez sur \"Edit Options\" et, sous \"Select a data access method\", sélectionnez l'option \"Customize\".**\n",
    "\n",
    "![](assets/M1_Earthdata_Search_SMAP-L3_customize_order.png)\n",
    "\n",
    "- **Faites défiler vers le bas jusqu'à \"Configure data customization options\" et jusqu'à \"Band subsetting.\"**\n",
    "- **Dans la zone de texte qui lit \"Filter\", tapez `soil_moisture_dca_pm`.** Cela filtrera les variables disponibles (\"bands\") pour n'afficher que cette variable spécifique, qui est l'estimation de l'humidité du sol basée sur l'algorithme à double canal (DCA) pour le passage après-midi (PM).\n",
    "- Pour s'assurer que nous téléchargeons uniquement les champs que nous voulons, **vous devrez décocher la case à côté de `SPL3SMP` puis recocher la case à côté de `soil_moisture_dca_pm` (voir capture d'écran ci-dessous) et chaque variable que nous voulons conserver.**\n",
    "\n",
    "![](assets/M1_Earthdata_Search_SMAP-L3_customize_order_variables.png)\n",
    "\n",
    "**Nous voulons télécharger les champs suivants (uniquement) :**\n",
    "\n",
    "- `soil_moisture_dca_pm`\n",
    "- `static_water_body_fraction_pm`\n",
    "- `retrieval_qual_flag_dca_pm`\n",
    "  \n",
    "Cliquez sur \"Done\" en bas de ce formulaire puis sur le gros bouton vert \"Download Data\" (Télécharger les Données) !"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b8d71ec-9442-451a-9db2-9445b5a5c44d",
   "metadata": {},
   "source": [
    "#### &#x1F6A9; <span style=\"color:red\">Attendez !</red>\n",
    "\n",
    "Parce que nous avons sélectionné un sous-ensemble de variables, nous devrons attendre de recevoir un e-mail nous indiquant que la commande est prête. **Vous n'avez pas besoin de faire ces étapes vous-même, car j'ai déjà préparé tous les granules de données qui seraient téléchargés de cette manière.** Ils peuvent être téléchargés directement ici :\n",
    "\n",
    "- [SMAP_L3_SPL3SMP_V008_20170601_20170930.zip](http://files.ntsg.umt.edu/data/ScienceCore/SMAP_L3_SPL3SMP_V008_20170601_20170930.zip) (Extrayez le contenu de ce fichier ZIP dans votre dossier `data_raw/SMAP_L3`)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a6fcfd0-56f9-4ad7-9998-4104e4680432",
   "metadata": {},
   "source": [
    "--- \n",
    "\n",
    "## Comprendre les fichiers de données hiérarchiques (HDF5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bae66ba-1f3f-4ead-a899-b68503ae252d",
   "metadata": {},
   "source": [
    "Les données SMAP de niveau 3 que nous avons téléchargées sont stockées dans un fichier de données hiérarchiques, version 5 (HDF5)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af777f84-05bf-4a48-b6d7-11ee827faf3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "filename = 'data_raw/SMAP_L3/SMAP_L3_SM_P_20170901_R18290_001.h5'\n",
    "hdf = h5py.File(filename, 'r')\n",
    "hdf"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adc21c83-44dc-4ef4-a7a6-dad557c0241c",
   "metadata": {},
   "source": [
    "Un fichier HDF5 ressemble beaucoup à un fichier netCDF4 : ce sont tous deux des fichiers hiérarchiques capables de stocker plusieurs ensembles de données et métadonnées divers dans un même fichier. Que voulons-nous dire par \"hiérarchique\" ? Eh bien, un fichier HDF5 ou netCDF4 est comme un arbre de fichiers, où les *ensembles de données* peuvent être organisés en différents *groupes* imbriqués, comme illustré ci-dessous. Les métadonnées, sous forme d'*attributs*, peuvent être attachées à n'importe quel ensemble de données ou groupe dans tout le fichier.\n",
    "\n",
    "![](assets/hdf5-structure.jpg)\n",
    "\n",
    "*Image fournie par NEON Science.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3b05df1-cebb-4fb9-b532-c41d542047c2",
   "metadata": {},
   "source": [
    "Nous pouvons consulter les groupes et les ensembles de données au plus haut niveau de cette hiérarchie en tapant :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f478567-664d-48cf-990a-f38ad78fa1c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae67bfe3-965a-41fd-9e0f-c3a798f759c2",
   "metadata": {},
   "source": [
    "L'objet `h5py.File`, `hdf`, est consulté comme un dictionnaire Python. Si nous voulons examiner le groupe `'Metadata'`, par exemple, nous tapons :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c73580d-bc31-4f59-8064-ccce0d0ef783",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a682e275-cd4b-49b2-aecf-61712aacec7a",
   "metadata": {},
   "source": [
    "Cela n'est pas très informatif, mais chaque groupe et ensemble de données dans un objet `h5py.File` se comporte également comme un dictionnaire Python :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9469661-4435-4472-b429-ccee11945eaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84eedb81-b0af-4ecf-8cee-46a70ca60fcd",
   "metadata": {},
   "source": [
    "Le groupe `'Metadata'` est un exemple de la manière dont nous pourrions stocker des informations dans un fichier HDF5 en dehors des tableaux multi-dimensionnels.\n",
    "\n",
    "Quelle est la signification de ce groupe vide ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55cad114-a67c-45d4-8941-e5914205bdf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata/ProcessStep']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "812a939d-338a-4dbe-a761-cf7976d4d308",
   "metadata": {},
   "source": [
    "Tout comme les fichiers netCDF, chaque ensemble de données dans un fichier HDF5 peut être étiqueté avec des attributs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dc84ca9-828b-411c-ad3b-4330e30fa509",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata/ProcessStep'].attrs.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66735587-3c8e-4572-a004-338c00222ebc",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Metadata/ProcessStep'].attrs['processor']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5573327d-a61f-45fe-8b82-9c2598767731",
   "metadata": {},
   "source": [
    "### Lecture des ensembles de données HDF5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ddfa9d8-3973-4632-9b86-8e5471e02dfe",
   "metadata": {},
   "source": [
    "Nous ouvrons un fichier HDF5 pour lecture avec le drapeau `'r'`, ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f23df3c-e8c9-4f72-8f6d-09b53342d914",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf = h5py.File(filename, 'r')\n",
    "hdf.keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9edc4fa-fbee-4f2a-89f7-8e7e31bf4144",
   "metadata": {},
   "source": [
    "Encore une fois, nous pouvons accéder aux ensembles de données de manière hiérarchique..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f44bc8d-9e70-4358-bdb8-3ffcc101ee26",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Soil_Moisture_Retrieval_Data_PM'].keys()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3e5406d-66b4-4860-8b6c-200f1b8752f2",
   "metadata": {},
   "source": [
    "Et si nous voulons lire un ensemble de données, nous utilisons la notation `[:]` de NumPy pour indiquer que nous voulons accéder à un tableau."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c20daf3-c174-460a-90c1-16d6d625fdb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf['Soil_Moisture_Retrieval_Data_PM/soil_moisture_dca_pm'][:]"

]
  },
  {
   "cell_type": "markdown",
   "id": "b7ed4753-4aef-4a43-9c3e-0a847cbdc8e4",
   "metadata": {},
   "source": [
    "Chaque fois que nous avons fini de travailler avec un fichier HDF5 ouvert, nous devons nous assurer de le fermer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4281dc17-0562-4902-b878-ba86687911fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "hdf.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e6cf24a-8164-49e9-aac7-d155880a3bcc",
   "metadata": {},
   "source": [
    "**Voyons ce qui est différent lorsque nous ouvrons le même fichier en utilisant `xarray`.** En particulier, examinez les **variables de données.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dd13e73-0f04-4ab1-9a15-ad5dcc0fa21b",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(filename)\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa81ced6-0b65-48b4-8290-04fada92528c",
   "metadata": {},
   "source": [
    "La seule variable trouvée, `\"crs\"`, ne sera pas très utile pour nous.\n",
    "\n",
    "**`xarray` a des limitations lorsqu'il ouvre des fichiers HDF5 ; il n'est pas capable de déterminer quels groupes sont disponibles.** Au lieu de cela, nous devons spécifier le groupe que nous voulons ouvrir."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4937b54-50e1-43de-b426-23745f86caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = xr.open_dataset(filename, group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eb4b6adc-f6ab-4ee8-8ac6-5503f7b9ec6b",
   "metadata": {},
   "source": [
    "Nous avons maintenant une variable utile, `\"soil_moisture_dca_pm\"`, mais notre `xarray.Dataset` n'a pas de coordonnées !\n",
    "\n",
    "Une manière de résoudre cela serait d'assigner des coordonnées à notre `xarray.Dataset`. C'est pourquoi nous avons besoin de la bibliothèque `h5py`, spécialisée dans la gestion des fichiers HDF5. Nous pouvons lire les coordonnées `\"x\"` et `\"y\"` à partir de notre `h5py.File` et les ajouter au `xarray.Dataset`, comme ci-dessous."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fbf531a-2dab-4067-8db3-5bdde91b4140",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instructeur : Notez l'assignation des coordonnées\n",
    "\n",
    "hdf = h5py.File(filename, 'r')\n",
    "ds = ds.assign_coords({'x': hdf['x'][:], 'y': hdf['y'][:]})\n",
    "hdf.close()\n",
    "ds"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83ad3ae9-9d89-4266-8c56-dcb28bee3054",
   "metadata": {},
   "source": [
    "Maintenant que nous avons à la fois une **variable de données** et des **coordonnées**, nous sommes prêts à tracer les données !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "87fa77a5-e5e3-4a5d-95ba-8c6fbd7a952d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize = (12, 5))\n",
    "ds['soil_moisture_dca_pm'].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9410de18-9d8f-40df-98ec-e8ac2452c6d9",
   "metadata": {},
   "source": [
    "#### &#x1F6A9; <span style=\"color:red\">Faites attention</red>\n",
    "\n",
    "**Remarquez les bandes dans cette image.** Le satellite SMAP a un temps de revisite de 2 à 3 jours. Cela signifie que, sur une seule journée, le radiomètre du satellite n'imagine qu'une partie du globe. Nous pourrions combiner les passages matinaux, `\"soil_moisture_dca_am\"`, et après-midi, `\"soil_moisture_dca_pm\"`, pour une seule journée, mais l'humidité du sol dans de nombreuses régions du monde varie beaucoup entre le matin et l'après-midi, donc cela pourrait ne pas être raisonnable.\n",
    "\n",
    "Nous avons choisi le passage après-midi, `\"soil_moisture_dca_pm\"`, car c'est généralement l'après-midi que le stress hydrique est le plus élevé dans les écosystèmes terrestres."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7b1d838-e3b8-4073-ac22-7e24324536d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_smap_l3(file_path):\n",
    "    '''\n",
    "    Paramètres\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Le chemin vers le fichier SMAP L3\n",
    "\n",
    "    Renvoie\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "    '''\n",
    "    with h5py.File(file_path, 'r') as hdf:\n",
    "        ds = xr.open_dataset(file_path, group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "        return ds.assign_coords({'x': hdf['x'][:], 'y': hdf['y'][:]})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee212fd6-14f6-429c-b4d5-7893179250e5",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Extraction des données SMAP L3\n",
    "\n",
    "Les données d'humidité du sol de SMAP sont globales, mais nous nous intéressons actuellement à une petite région d'étude, les plaines du nord des États-Unis. Comment pouvons-nous extraire les données SMAP pour notre zone d'intérêt ?\n",
    "\n",
    "Vous avez peut-être remarqué que les coordonnées que nous avons ajoutées à notre `xarray.Dataset`, ci-dessus, n'étaient pas des coordonnées latitude-longitude. [Les données SMAP sont projetées sur une grille EASE-Grid 2.0, où \"EASE\" signifie Equal-Area Scalable Earth.](https://nsidc.org/data/user-resources/help-center/guide-ease-grids#anchor-9-km-resolution-ease-grids) Cette projection unique, mondiale, présente de nombreux avantages, mais les coordonnées X (\"Easting\") et Y (\"Northing\") peuvent être difficiles à comprendre lorsque nous avons l'habitude de travailler avec des coordonnées latitude-longitude."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f756993-2ee8-4846-85ab-2aa46668ff2d",
   "metadata": {},
   "source": [
    "**Ci-dessous est un exemple de la façon dont nous pourrions modifier notre fonction `process_smap_l3()` pour convertir les coordonnées de notre ensemble de données des coordonnées EASE-Grid 2.0 en coordonnées longitude et latitude à l'aide de la bibliothèque `pyproj`.**\n",
    "\n",
    "```python\n",
    "from pyproj import Transformer\n",
    "from pyproj import CRS\n",
    "\n",
    "def process_smap_l3(file_path):\n",
    "    '''\n",
    "    Paramètres\n",
    "    ----------\n",
    "    file_path : str\n",
    "        Le chemin vers le fichier SMAP L3\n",
    "\n",
    "    Renvoie\n",
    "    -------\n",
    "    xarray.Dataset\n",
    "    '''\n",
    "    # Obtenir une fonction pour convertir les coordonnées de l'EASE-Grid 2.0 en WGS84 (latitude-longitude)\n",
    "    crs_ease2 = CRS.from_epsg(6933)\n",
    "    crs_wgs84 = CRS.from_epsg(4326)\n",
    "    transform = Transformer.from_crs(crs_ease2, crs_wgs84)\n",
    "    \n",
    "    with h5py.File(file_path, 'r') as hdf:\n",
    "        # Convertir de (Northing, Easting) en (Latitude, Longitude)\n",
    "        lng = list(map(lambda x: transform.transform(x, 0)[1], hdf['x'][:].tolist()))\n",
    "        lat = list(map(lambda y: transform.transform(0, y)[0], hdf['y'][:].tolist()))\n",
    "        ds = xr.open_dataset(file_path, group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "        \n",
    "    return ds.assign_coords({'x': lng, 'y': lat})\n",
    "```\n",
    "\n",
    "&#x1F449; **Pour simplifier, nous ne ferons pas cette conversion des coordonnées dans la leçon d'aujourd'hui.** \n",
    "\n",
    "**Nous allons plutôt extraire nos données en utilisant une plage de lignes et de colonnes correspondant à une zone près de Glasgow, dans le Montana : 105.882 W de longitude et 48.059 N de latitude.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c780896-f79e-493c-bbfa-99c049824b28",
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = process_smap_l3('data_raw/SMAP_L3/SMAP_L3_SM_P_20170802_R18290_001.h5')\n",
    "\n",
    "ds['soil_moisture_dca_pm'][49:59,187:227].plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f2f9dd8-0a75-493e-ad70-08423c41c152",
   "metadata": {},
   "source": [
    "#### &#x1F6A9; <span style=\"color:red\">Faites attention</red>\n",
    "\n",
    "Il y a deux points à noter sur cette image :\n",
    "\n",
    "- **Il est évident que la partie ouest de notre zone d'étude a été manquée par le satellite ce jour-là.** Cela signifie que, selon le jour, la valeur reflète les conditions d'humidité du sol dans différentes parties plus petites de notre région d'étude.\n",
    "- **Il y a une zone avec une très forte humidité du sol (pixel jaune vif) au centre gauche de l'image.** Ce pixel est presque certainement un plan d'eau permanent, donc si nous faisions une analyse régionale, nous voudrions le masquer.\n",
    "\n",
    "Abordons ces deux problèmes."
   ]
  },
 {
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7424a44b-2747-4bc7-9dba-b25f3d470240",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Masquage des plans d'eau permanents"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "01f5312a-ad29-4f7f-bf64-7f0aa2591179",
      "metadata": {},
      "source": [
        "Jetons un coup d'œil à l'ensemble de données `static_water_body_fraction_pm`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a02a8471-1170-47b4-bea7-09dbddaa767e",
      "metadata": {},
      "outputs": [],
      "source": [
        "pyplot.figure(figsize = (12, 5))\n",
        "ds['static_water_body_fraction_pm'].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "683453a8-2543-4fb1-af6b-dc39e7b72b5c",
      "metadata": {},
      "source": [
        "Bien que la fraction de surface couverte par l'eau soit \"statique\" et ne change pas au fil du temps, l'ensemble de données SMAP L3 ne montre que la partie du masque où les données ont été acquises, d'où l'effet de bandes.\n",
        "\n",
        "Concentrons-nous maintenant sur notre zone d'étude."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e093541f-8ad6-4322-b473-d88bed88278b",
      "metadata": {},
      "outputs": [],
      "source": [
        "ds['static_water_body_fraction_pm'][49:59,187:227].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "a13c8783-deba-46e0-aa81-08761f58039b",
      "metadata": {},
      "source": [
        "Comment pourrions-nous utiliser cela pour masquer les plans d'eau permanents dans nos données d'humidité du sol ? Nous devons décider du seuil de couverture en eau fractionnaire à utiliser pour définir les plans d'eau permanents. Adoptons la convention selon laquelle les pixels avec plus de 20 % de leur surface couverte par de l'eau (c'est-à-dire une couverture fractionnaire de $\\ge 0.2$) doivent être masqués.\n",
        "\n",
        "L'image ci-dessus nous rappelle un autre problème à traiter : chaque image quotidienne peut ne contenir qu'une partie de notre zone d'étude. **Dans chaque granule quotidien, nous pouvons utiliser l'ensemble de données `static_water_body_fraction_pm` pour créer un masque binaire où `1` indique un plan d'eau permanent et `0` indique tout le reste (données valides).**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ebaf9779-38bf-4216-bc46-9743f2185cf2",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.where(ds['static_water_body_fraction_pm'][:] >= 0.2, 1, 0)[49:59,187:227]"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "644ef595-e11f-48f0-a327-a69d5c34e0b7",
      "metadata": {},
      "source": [
        "Mettons à jour notre fonction `process_smap_l3()` pour inclure le masquage des données. **Notez que nous avons ajouté un argument de mot-clé à notre fonction pour permettre aux utilisateurs de contrôler le seuil de couverture en eau fractionnaire utilisé. En fournissant un argument par défaut à `threshold`, nous documentons également implicitement notre décision d'utiliser `0.2` comme seuil.**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "98502d44-682f-4dd3-b44d-b7a4b0a749fe",
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_smap_l3(file_path, threshold = 0.2):\n",
        "    '''\n",
        "    Paramètres\n",
        "    ----------\n",
        "    file_path : str\n",
        "        Le chemin vers le fichier SMAP L3\n",
        "    threshold : float\n",
        "        Le seuil de couverture en eau fractionnaire utilisé pour masquer\n",
        "        les plans d'eau permanents\n",
        "\n",
        "    Renvoie\n",
        "    -------\n",
        "    xarray.Dataset\n",
        "    '''\n",
        "    with h5py.File(file_path, 'r') as hdf:\n",
        "        ds = xr.open_dataset(file_path, group = 'Soil_Moisture_Retrieval_Data_PM')\n",
        "        ds = ds.assign_coords({'x': hdf['x'][:], 'y': hdf['y'][:]})\n",
        "\n",
        "    # Créer un tableau binaire (0 ou 1) basé sur le seuil de couverture en eau\n",
        "    mask = np.where(ds['static_water_body_fraction_pm'][:] >= threshold, 1, 0)\n",
        "    \n",
        "    # Lire l'ensemble de données sous forme de tableau NumPy pour pouvoir le masquer\n",
        "    data = ds['soil_moisture_dca_pm'].to_numpy()\n",
        "    \n",
        "    # Écrire NaN dans cet ensemble de données là où se trouvent des plans d'eau permanents\n",
        "    data[mask == 1] = np.nan\n",
        "    ds['soil_moisture_dca_pm'][:] = data\n",
        "    return ds"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9def7a50-3b54-4ac6-9a8d-61837cc64ece",
      "metadata": {},
      "outputs": [],
      "source": [
        "ds = process_smap_l3('data_raw/SMAP_L3/SMAP_L3_SM_P_20170802_R18290_001.h5')\n",
        "ds['soil_moisture_dca_pm'][49:59,187:227].plot()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d0aa913a-04ed-46e1-b2f5-2e8ffe8065cc",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Utilisation des drapeaux de qualité d'assurance (QA)\n",
        "\n",
        "Une autre considération lors de l'utilisation des ensembles de données d'observation de la Terre par satellite est l'information sur l'**assurance qualité (QA).** Parfois, un capteur satellite échoue à obtenir une bonne valeur pour un pixel donné, peut-être en raison de conditions nuageuses (dans le cas d'un capteur optique) ou à cause d'une défaillance temporaire de l'instrument ou du logiciel.\n",
        "\n",
        "Cette information est généralement stockée sous forme de bande séparée dans un ensemble de données raster multibande ou, dans ce cas, comme un ensemble de données distinct dans un fichier hiérarchique. L'ensemble de données QA a une valeur pour chaque pixel dans l'image et cette valeur transmet différents types d'informations sur la qualité de la récupération des capteurs satellitaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f4474b1-951d-4e21-b7c7-a018b445265e",
      "metadata": {},
      "outputs": [],
      "source": [
        "flags = ds['retrieval_qual_flag_dca_pm'].to_numpy()\n",
        "\n",
        "# Obtenir chaque valeur unique dans cet ensemble de données\n",
        "np.unique(flags)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1d7b089c-1e3c-4347-b41b-d29d025dcc05",
      "metadata": {},
      "source": [
        "**Que signifient ces nombres ?** Chaque nombre décimal ci-dessus est en réalité une représentation *compressée* de plusieurs drapeaux QA. Chaque drapeau QA est un `1` ou un `0`, correspondant à une représentation binaire. Voici un exemple de ce à quoi cela ressemble, tiré de la mission Moderate Resolution Imaging Spectroradiometer (MODIS) de la NASA :\n",
        "\n",
        "![](./assets/bit-packing.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cc47ebde-4998-4c29-94b7-fe8e79970fbb",
      "metadata": {},
      "source": [
        "L'exemple ci-dessus montre une représentation binaire du nombre 64. Nous savons que le nombre décimal correspondant est 64 car, ci-dessus, il n'y a qu'une seule valeur `1` et elle se trouve à la *sixième* position en partant de la droite. En utilisant la base 2 (car il y a deux chiffres binaires) et en l'élevant à la puissance six, nous obtenons 64."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "13689036-a085-4e06-b127-4b9b73bb16fd",
      "metadata": {},
      "outputs": [],
      "source": [
        "2**6"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0cc47816-51c3-413d-894d-33ce1be16a90",
      "metadata": {},
      "source": [
        "En d'autres termes, voici comment nous convertissons le nombre binaire `01000000` (ci-dessus) en nombre décimal :\n",
        "\n",
        "$$\n",
        "64 = (0\\times 2^7) + (1\\times 2^6) + (0\\times 2^5) + (0\\times 2^4) + (0\\times 2^3) + (0\\times 2^2) + (0\\times 2^1) + (0\\times 2^0)\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8d428a92-1651-4bd0-9b21-e8093d7648c2",
      "metadata": {},
      "source": [
        "Chaque chiffre binaire est un *bit.* Dans l'exemple ci-dessus, des groupes d'un ou plusieurs bits (bitwords) correspondent à différentes catégories d'informations QA. Dans cet exemple MODIS, pour le bitword numéro 2 (\"Bit No.2\"), un `0` indique que \"les détecteurs semblent en bon état\" tandis qu'un `1` indiquerait le contraire.\n",
        "\n",
        "**Comment pouvons-nous utiliser cette information ?** Eh bien, tout d'abord, nous devrions consulter [le guide de l'utilisateur de SMAP L3](https://nsidc.org/sites/default/files/spl3smp-v008-userguide.pdf) et consulter le tableau 6. Le guide de l'utilisateur est disponible sur [la page produit SMAP L3 du site NSIDC.](https://nsidc.org/data/spl3smp/versions/8)\n",
        "\n",
        "Le guide de l'utilisateur pour ce produit et d'autres produits de la NASA nous indique ce que représente chaque bitword. Pour notre cas d'utilisation, nous ne voulons utiliser que les données SMAP L3 où \"la récupération de l'humidité du sol a une qualité recommandée\", ce qui, comme nous le voyons dans le tableau 6, signifie que le bit le plus à droite doit être zéro.\n",
        "\n",
        "**Comment pouvons-nous savoir quels nombres décimaux dans l'ensemble de données `retrieval_qual_flag_dca_pm` correspondent à cette condition (où le bit le plus à droite est zéro) ?** Eh bien, d'après la formule ci-dessus, où nous avons converti le binaire en décimal, nous pouvons noter que chaque nombre *impair* doit avoir un `1` dans le bit le plus à droite, car c'est la seule façon d'obtenir un nombre impair dans la somme :\n",
        "\n",
        "$$\n",
        "1 = 1\\times 2^0\n",
        "$$\n",
        "\n",
        "**Par conséquent, seuls les nombres décimaux pairs correspondent à la situation où \"la récupération de l'humidité du sol a une qualité recommandée.\"** Nous pouvons vérifier cela ci-dessous en utilisant la fonction `np.unpackbits()`, qui convertit les nombres décimaux en représentations binaires."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d1b45a89-7510-4a90-ba51-dd58abba6c82",
      "metadata": {},
      "outputs": [],
      "source": [
        "flags = np.unique(ds['retrieval_qual_flag_dca_pm'].to_numpy())\n",
        "\n",
        "flags = flags.astype(np.uint8).reshape((flags.size, 1))\n",
        "flags"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc5b4bc-ff41-4ca7-819a-eefbd8cc422d",
      "metadata": {},
      "outputs": [],
      "source": [
        "np.unpackbits(flags, axis = 1)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "061da4de-d10f-461d-99c8-c1d47c8d12aa",
      "metadata": {},
      "source": [
        "**Pourquoi est-ce si compliqué ?** L'utilisation de drapeaux QA compressés en bits est un héritage de l'époque où l'espace disque était coûteux et où les fichiers devaient être aussi petits que possible. Aujourd'hui, dans les ensembles de données hiérarchiques comme les fichiers HDF5, nous pouvons tirer parti de la *compression de fichiers* pour les rendre plus petits. Mais certaines habitudes sont difficiles à changer, et de nombreux ensembles de données de la NASA utilisent encore la compression en bits pour stocker des informations QA.\n",
        "\n",
        "L'opérateur de modulo, `%`, peut nous dire si un nombre est pair ou impair, car il donne le *reste* de la division entière."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f9271e02-e740-468e-98cc-2953bec11b9e",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Quels entiers sont divisibles par 2 ?\n",
        "np.array([3, 8, 16]) % 2"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "52323697-6218-4325-8d9b-e2a3c611fb98",
      "metadata": {},
      "source": [
        "Mettons à jour à nouveau notre fonction `process_smap_l3()`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ff36c44c-512e-4b6d-9b0f-2f890817cc64",
      "metadata": {},
      "outputs": [],
      "source": [
        "def process_smap_l3(file_path, threshold = 0.2):\n",
        "    '''\n",
        "    Paramètres\n",
        "    ----------\n",
        "    file_path : str\n",
        "        Le chemin vers le fichier SMAP L3\n",
        "    threshold : float\n",
        "        Le seuil de couverture en eau fractionnaire utilisé pour masquer\n",
        "        les plans d'eau permanents\n",
        "\n",
        "    Renvoie\n",
        "    -------\n",
        "    xarray.Dataset\n",
        "    '''\n",
        "    with h5py.File(file_path, 'r') as hdf:\n",
        "        ds = xr.open_dataset(file_path, group = 'Soil_Moisture_Retrieval_Data_PM')\n",
        "        ds = ds.assign_coords({'x': hdf['x'][:], 'y': hdf['y'][:]})\n",
        "\n",
        "    # Créer un tableau binaire (0 ou 1) basé sur le seuil de couverture en eau\n",
        "    water_mask = np.where(ds['static_water_body_fraction_pm'][:] >= threshold, 1, 0)\n",
        "    # Créer un tableau binaire basé sur les informations QA\n",
        "    qa_mask = np.where(ds['retrieval_qual_flag_dca_pm'][:] % 2 != 0, 1, 0)\n",
        "    \n",
        "    # Lire l'ensemble de données sous forme de tableau NumPy pour pouvoir le masquer\n",
        "    data = ds['soil_moisture_dca_pm'].to_numpy()\n",
        "    \n",
        "    # Écrire NaN dans cet ensemble de données là où se trouvent des plans d'eau permanents\n",
        "    data[water_mask == 1] = np.nan\n",
        "    # Et également masquer les valeurs de faible qualité\n",
        "    data[qa_mask == 1] = np.nan\n",
        "    ds['soil_moisture_dca_pm'][:] = data\n",
        "    return ds"
      ]
    },
   {
   "cell_type": "markdown",
   "id": "0ac1dc89-86d7-4aff-bc24-9f24b35b2202",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Création d'une série temporelle de l'humidité du sol"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e907b6e-9fdd-4099-9098-be798bd53e34",
   "metadata": {},
   "source": [
    "Ce serait bien si nous pouvions utiliser `xr.open_mf_dataset()` pour ouvrir tous ces fichiers HDF5 de SMAP comme un seul ensemble de données de série temporelle. Cependant, si nous essayons cela, nous constaterons que cela ne fonctionne pas car `xarray` ne sait pas quelles sont les coordonnées d'un ensemble de données HDF5, donc il ne peut pas combiner les ensembles de données ensemble.\n",
    "\n",
    "```python\n",
    "ds = xr.open_mfdataset('data_raw/SMAP_L3/*.h5', group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "```\n",
    "```\n",
    "---------------------------------------------------------------------------\n",
    "ValueError                                Traceback (most recent call last)\n",
    "Cell In[166], line 1\n",
    "----> 1 ds = xr.open_mfdataset('data_raw/SMAP_L3/*.h5', group = 'Soil_Moisture_Retrieval_Data_PM')\n",
    "\n",
    "...\n",
    "\n",
    "ValueError: Impossible de trouver des coordonnées de dimension pour ordonner les ensembles de données en vue de la concaténation\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec8915f6-1620-42f8-ac12-0073d8c722bf",
   "metadata": {},
   "source": [
    "**Cela signifie que nous devrons ouvrir chaque fichier nous-mêmes et empiler les tableaux ensemble.**\n",
    "\n",
    "Nous pouvons utiliser la bibliothèque `glob` pour obtenir une liste de tous les fichiers que nous voulons. La notation ci-dessous, `'data_raw/SMAP_L3/*.h5'`, est similaire à celle que nous avons utilisée avec `xr.open_mfdataset()`: nous indiquons que nous voulons utiliser tous les fichiers HDF5 dans un répertoire particulier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83708eb4-2559-42a4-889a-55d34041ef4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob\n",
    "\n",
    "file_list = glob.glob('data_raw/SMAP_L3/*.h5')\n",
    "\n",
    "# Obtenez le premier nom de fichier\n",
    "file_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a909bd02-cb62-4509-9dfa-d3d099d5d8bd",
   "metadata": {},
   "source": [
    "#### &#x1F6A9; <span style=\"color:red\">Attention</red>\n",
    "\n",
    "**Lorsque nous utilisons `glob.glob()` pour des fichiers représentant une série temporelle, il est très important de nous assurer que les fichiers sont listés dans l'ordre chronologique !**\n",
    "\n",
    "Tant que les noms de fichiers incluent un horodatage cohérent, comme une date au format `AAAAMMJJ` (Année-Mois-Jour), nous pouvons utiliser la méthode `sort()` de la liste Python pour obtenir les fichiers dans un ordre alphanumérique, ce qui correspond dans ce cas à l'ordre chronologique."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "559e44bd-acb6-4622-ba67-97861f278ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_list.sort()\n",
    "file_list[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b6d60b3-b781-4821-9754-13981dcba982",
   "metadata": {},
   "source": [
    "**Écrivons une boucle `for` pour traiter chaque granule SMAP L3 et extraire la valeur estimée de l'humidité du sol dans notre zone d'intérêt (105,882 W, 48,059 N).**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e74b5c0-d902-497a-8207-a7dcd69aa618",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Les coordonnées ligne, colonne de Glasgow, Montana ; elles pourraient\n",
    "#   être dérivées en utilisant la bibliothèque \"pyproj\" et une transformation affine\n",
    "row, column = (51, 198)\n",
    "\n",
    "sm_mean = []\n",
    "for filename in file_list:\n",
    "    ds = process_smap_l3(filename, threshold = 0.2)\n",
    "    sm_mean.append(ds['soil_moisture_dca_pm'][row,column])\n",
    "\n",
    "sm_mean = np.hstack(sm_mean)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "028f5c9b-c751-4abd-b3fc-117a3a8fcb7c",
   "metadata": {},
   "source": [
    "Lorsque nous traçons les données, il serait intéressant d'afficher les dates sur l'axe horizontal. Nous pouvons obtenir une séquence de dates à l'aide de la fonction `pandas.date_range()` [(documentée ici)](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.date_range.html).\n",
    "\n",
    "Nous voudrons également combler les valeurs manquantes dans notre série temporelle pour faciliter la visualisation. **Le remplissage anticipé** est une approche simple : de la première à la dernière valeur, lorsque nous rencontrons une valeur manquante, nous répétons la dernière valeur non manquante jusqu'à rencontrer la prochaine valeur non manquante. Les méthodes `ffill()` et `bfill()` d'un `DataFrame` de `pandas` facilitent cette tâche."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d6743a-4396-43e9-b7f6-883e0c61d0f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "\n",
    "dates = pandas.date_range('2017-06-01', '2017-09-30', freq = '1D')\n",
    "df = pandas.DataFrame({'date': dates, 'SM': sm_mean})\n",
    "\n",
    "# Remplissage anticipé des valeurs manquantes d'humidité du sol (SM), suivi d'un remplissage rétroactif\n",
    "df['SM_filled'] = df['SM'].ffill().bfill()\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7f3aa3-8f7e-44c6-925a-d7f9c98cc973",
   "metadata": {},
   "source": [
    "Enfin, nous sommes prêts à tracer les données !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb4fe3b4-5344-450e-a86a-98b9006a2738",
   "metadata": {},
   "outputs": [],
   "source": [
    "pyplot.figure(figsize = (10, 5))\n",
    "pyplot.plot(df['date'], df['SM_filled'], 'k-')\n",
    "pyplot.ylabel('Humidité volumétrique du sol (m3 m-3)')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2eff8b45-36c9-41dd-8c80-bb209f84cd64",
   "metadata": {},
   "source": [
    "Les pics peuvent sembler étranges au début, mais ils sont très similaires à ce que nous attendrions pour des événements pluvieux, suivis d'un assèchement rapide. Nous pouvons vérifier si ces pics correspondent à des événements pluvieux en traçant [des précipitations connues près de Wolf Point, Montana](https://www.weather.gov/wrh/Climate?wfo=ggw) (la station météorologique la plus proche de notre zone d'intérêt)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1f564e-5fba-4111-b120-e53007e8c2c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import datetime\n",
    "\n",
    "precip_events = [\n",
    "    datetime.date(2017, 6, 12),\n",
    "    datetime.date(2017, 7, 11),\n",
    "    datetime.date(2017, 8, 2),\n",
    "    datetime.date(2017, 9, 15)\n",
    "]\n",
    "\n",
    "pyplot.figure(figsize = (10, 5))\n",
    "pyplot.vlines(precip_events, ymin = df['SM_filled'].min(), ymax = df['SM_filled'].max())\n",
    "pyplot.plot(df['date'], df['SM_filled'], 'k-', alpha = 0.6)\n",
    "pyplot.ylabel('Humidité volumétrique du sol (m3 m-3)')\n",
    "pyplot.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cce28d28-4459-473e-bb09-fef56f8cd6f6",
   "metadata": {},
   "source": [
    "Certains pics ne sont pas associés à des événements pluvieux connus à cette station, mais nous devons nous rappeler que cette station météorologique ne représente qu'une petite partie de ces pixels de 36 km² ; il est possible que de la pluie soit tombée quelque part dans cette région de 36 km², mais pas à la station météorologique elle-même.\n",
    "\n",
    "La période associée à la sécheresse éclair (début septembre) ne semble pas particulièrement sèche, comparée aux dates précédentes, bien que les pics d'humidité du sol soient plus bas. Encore une fois, cette station météorologique pourrait ne pas être représentative des conditions régionales."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c64bda8-8d1d-4ef1-87a9-5fed52fd8a56",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Résumé\n",
    "\n",
    "- NASA Earthdata Search propose de nombreux ensembles de données pour étudier le système climatique de la Terre. Les ensembles de données de réanalyse et les ensembles de données de niveau 4 de la NASA intègrent plusieurs sources de données brutes pour fournir un enregistrement continu et une couverture spatiale complète, sans lacunes. Cependant, ils peuvent inclure des biais de modèles qui ne reflètent pas les conditions réelles. Les ensembles de données de télédétection et les ensembles de données de niveau 3 de la NASA offrent des observations plus directes.\n",
    "\n",
    "- **Documentez vos données !** Lorsque vous téléchargez des données brutes, gardez-les séparées et non modifiées. Assurez-vous de créer un fichier `README`, placé dans le même répertoire ou dans le répertoire parent, contenant des informations sur l'origine des données et leur utilisation.\n",
    "\n",
    "- **Documentez votre processus !** Lors de l'utilisation des données, des fonctions Python réutilisables peuvent accélérer votre travail. Des fonctions bien écrites servent également de documentation de votre flux de travail, car elles décrivent les étapes que vous avez suivies pour traiter les données.\n",
    "\n",
    "- **Les fichiers HDF5 et netCDF4 peuvent vous aider à rester organisé.** Les deux formats de fichiers vous permettent d'ajouter des *attributs* aux ensembles de données qu'ils contiennent, ce qui est un bon moyen de documenter les unités de mesure, les sources de données originales et d'autres *métadonnées* clés.\n",
    "\n",
    "### Lecture des fichiers HDF5 et netCDF4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b387aa-ec14-4496-a012-8bceff9b602c",
   "metadata": {},
   "source": [
    "|                            |  Fichiers HDF5                        | Fichiers netCDF4                        | `xarray` (pour les deux)        |\n",
    "|:---------------------------|:--------------------------------------|:----------------------------------------|:-------------------------------|\n",
    "|Module d'importation         | `import h5py`                         | `import netCDF4`                        | `import xarray as xr`           |\n",
    "|Fichiers ouverts avec...     | `hdf = h5py.File(...)`                | `nc = netCDF4.Dataset()`                | `ds = xr.open_dataset()`        |\n",
    "|Ensembles/groupes consultés...| `hdf.keys()`                         | `nc.variables` ou `nc.variables.keys()` | `list(ds.variables.keys())`     |\n",
    "|                            | `hdf['group_name'].keys()`            | `nc.variables['group_name'].keys()`     |                                |\n",
    "|Accès aux ensembles via...   | `hdf`                                | `nc.variables`                          | `ds.variables`                 |\n",
    "|Attributs listés via...      | `hdf.attrs`                          | `nc.ncattrs()`                          | `ds.attrs`                     |\n",
    "|                            | `hdf['dataset'].attrs`                | `nc.variables['dataset'].ncattrs()`     |                                |\n",
    "|Lecture des attributs via... | `hdf['dataset'].attrs['attribute']`   | `nc.variables['dataset'].getncattr()`   | `ds.variables['dataset']`      |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d70f04ad-61cd-40c5-8db1-a218161b48ee",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### Ressources supplémentaires\n",
    "\n",
    "- [En savoir plus sur les grilles Equal-Area Scalable Earth (EASE) ici.](https://nsidc.org/data/user-resources/help-center/guide-ease-grids#anchor-9-km-resolution-ease-grids)\n",
    "- Vous vous demandez comment utiliser `earthaccess.open()` avec `xarray` pour ne pas avoir à conserver de fichiers téléchargés ? Eh bien, `xarray.open_dataset()` peut être lent lorsque vous avez beaucoup de fichiers à ouvrir, comme dans cet exemple de série temporelle. [Cet article décrit comment vous pouvez accélérer `xarray.open_dataset()`](https://climate-cms.org/posts/2018-09-14-dask-era-interim.html) lorsque vous travaillez avec plusieurs fichiers hébergés dans le cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db7ae193-7a05-4bde-8264-209483e74743",
   "metadata": {},
   "source": [
    "### Références\n",
    "\n",
    "- Chen, L. G., J. Gottschalck, A. Hartman, D. Miskus, R. Tinker, and A. Artusa. 2019. Flash drought characteristics based on U.S. Drought Monitor. Atmosphere 10 (9):498.\n",
    "- He, M., J. S. Kimball, Y. Yi, S. W. Running, K. Guan, K. Jensco, B. Maxwell, and M. Maneta. 2019. Impacts of the 2017 flash drought in the US Northern plains informed by satellite-based evapotranspiration and solar-induced fluorescence. Environmental Research Letters 14 (7):074019."
   ]
  }
  ],
 "metadata": {
        "kernelspec": {
            "display_name": "Python 3",
            "language": "python",
            "name": "python3"
        },
        "language_info": {
            "name": "python",
            "version": "3.8.5",
            "mimetype": "text/x-python",
            "file_extension": ".py",
            "codemirror_mode": {
                "name": "ipython",
                "version": 3
            },
            "pygments_lexer": "ipython3",
            "nbconvert_exporter": "python"
        }
    }
}
